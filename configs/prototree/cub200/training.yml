param_groups:
  backbone_to_freeze:
    stop: extractor.convnet.layer4.1 # None/null means first named parameter, second name is inclusive
  backbone_to_train: extractor.convnet.layer4.2
  add_on_layers: extractor.add_on # All named parameters starting with this prefix
  prototypes: classifier.prototypes
  #leaves_distributions: classifier.tree

optimizers:
  main_optimizer:
    type: AdamW
    groups:
      backbone_to_freeze:
        lr: 0.00001
        weight_decay_rate: 0.0
      backbone_to_train:
        lr: 0.001
        weight_decay_rate: 0.0
      add_on_layers:
        lr: 0.001
        weight_decay_rate: 0.0
      prototypes:
        lr: 0.001
        weight_decay_rate: 0
    params:
      lr: 0.001
      eps: 0.0000001
    scheduler:
      type: MultiStepLR # or ReduceLROnPlateau
      params:
        milestones: [60, 70, 80, 90, 100]
        gamma: 0.5

num_epochs: 100

# By default, everything that is not frozen is trained
periods:
  warmup:
      epoch_range: [ 0, 29 ] # First and last epoch (included)
      freeze: [ backbone_to_freeze ]
      optimizers:
        - main_optimizer



# After training
epilogue:
  pruning_threshold: 0.01
  projection_file: projection_info.csv