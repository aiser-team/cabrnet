from __future__ import annotations
from typing import Callable, Dict, Tuple, Union

from loguru import logger
import numpy as np
import onnx
import onnx_graphsurgeon as gs
import onnxruntime as rt
import torch
import torch.nn as nn
from torchvision.models._api import register_model


from pathlib import PurePath

__all__ = ["generic_onnx_model", "GenericONNXModel"]

"""
TODO:
* [x] path sanitization and file permissions for ONNX models generation
      (the ONNX file handler class)
* [x] Saving generated ONNX models
* [ ] Copying generated ONNX models with the rest of the configuration
      once generated to enable reproduceability
* [x] Create a collection of ORT running sessions with provided layer keys,
      mirroring the ConvExtractor class capability to provide multi-output
      models
* [ ] type sanitization conversions for the ONNX sessions bindings
"""


class ONNXVariantsHandler:
    r"""A class handling multiple variations of a base ONNX file.

    Attributes:
        original_path (str): the original path of provided ONNX file. Not
        modified during inference.

        variants (dict[str, onnx.ModelProto]): A dict[str,onnx.ModelProto]
        in charge of storing the variants of ONNX generated by this class.
        Key is a pipeline name.

    """

    def __init__(self, original_path: PurePath):
        self.original_name = "original"
        self.original_path = original_path
        self.variants: Dict[str, onnx.ModelProto] = {}

    def register_and_save_variant(self, name: str, model: onnx.ModelProto):
        """Register a variant from a name and a model. Perform ONNX saving."""
        if name not in self.variants:
            self.variants[name] = model
            savepath = PurePath("_".join([PurePath(self.original_path).stem, name, ".onnx"]))
            onnx.save_model(model, savepath)
            logger.info(f"Saved result of morphed ONNX at {savepath}")
        else:
            logger.warning(f"Error, variant {name} already registered, doing nothing")
            pass

    def get_variant_path(self, name: str) -> PurePath:
        if name in self.variants:
            return PurePath("_".join([PurePath(self.original_path).stem, name, ".onnx"]))
        else:
            logger.error(
                f"Warning, ONNX variant {name} not registered in ONNX handler. Trying to load non-existing file. Aborting "
            )
            raise FileNotFoundError

    def get_variant_model(self, name):
        """Get a model from a variant name."""
        if name in self.variants:
            return self.variants[name]
        else:
            logger.error(
                f"Warning, ONNX variant {name} not registered in ONNX handler. Trying to load non-existing model. Aborting "
            )

    def safe_onnx_compute(self, f: Callable[..., onnx.ModelProto], variant_name: str, **kwargs):
        r"""Given a function f that performs a modification on the original
            model, compute f and automatically register the result.

        Args:
            f (Callable): Function that transforms an ONNX model. Assumed to
            return an ONNX model.

            variant (str): the variant name to save

            kwargs: Arguments of f
        """

        model = f(**kwargs)
        self.register_and_save_variant(name=variant_name, model=model)

    def __iter__(self):
        self.iter = iter(self.variants.items())
        return self.iter

    def get_only_modified_variants(self):
        # Filtering out the original backbone is sometimes necessary
        d = {k: v for k, v in self.variants.items() if k != self.original_name}
        return d


class GenericONNXModel(nn.Module):
    r"""A class describing generic ONNX models to be used as backbone.

    This class provides a forward method to use ONNX models inside CaBRNet,
    relying on the onnxruntime. It does not support training.

    If provided with a dictionary of layers, the class handles the
    generation of alternate ONNX models describing
    the original model, trimed upto a specific given layer.
    This mirrors the original ConvExtractor construction.

    Some assumptions about the loaded ONNX model:
        * it does not have any cycles
        * it does not have any control structures like conditional or loops
        * first dimensions of input shape and output shape are symbolic


    Attributes:
        variants: A ONNXVariantsHandler object.
    """

    def __init__(self, onnx_path: str):
        super(GenericONNXModel, self).__init__()
        onnx_purepath = PurePath(onnx_path)
        self.variants = ONNXVariantsHandler(original_path=onnx_purepath)
        model = onnx.load(onnx_purepath)
        onnx.checker.check_model(model)
        model = onnx.shape_inference.infer_shapes(model)
        logger.info(f"Loaded ONNX model located at {onnx_purepath} and performed sanity checks on it.")
        self.variants.register_and_save_variant(self.variants.original_name, model)

    def get_output_shape_of_node(self, node: onnx.ValueInfoProto) -> Tuple[Union[int, str], int, int, int]:
        assert len(node.type.tensor_type.shape.dim) == 4
        [b, x, y, z] = node.type.tensor_type.shape.dim
        if hasattr(b, "dim_param"):
            b_v = b.dim_param
        else:
            b_v = b.dim_value
        out_shape = (b_v, x.dim_value, y.dim_value, z.dim_value)
        return out_shape

    def get_output_shape_of_layer(
        self, model: onnx.ModelProto, layer_cut: str
    ) -> Tuple[Union[int, str], int, int, int]:
        # shape inference must have been run beforehand
        assert len(model.graph.value_info) > 0
        node_candidates = [x for x in model.graph.value_info if x.name.__contains__(layer_cut)]
        # otherwise, layer_cut is underspecified
        assert len(node_candidates) == 1
        return self.get_output_shape_of_node(node_candidates[0])

    def _trim_model(self, model: onnx.ModelProto, layer_cut: str) -> onnx.ModelProto:
        r"""
        Trim the original ONNX graph upto
        the provided layer name (included), and saves
        the corresponding ONNX graph on disk.
        Returns both the modified model and the path of the saved ONNX file.

        Args:
            model (onnx.ModelProto): ONNX model to trim.
            layer_cut (str): Layer to trim the model to.


        """
        logger.info(f"Performing ONNX model edition to provide an alternate output.")
        to_trim = False
        out_shape = self.get_output_shape_of_layer(model=model, layer_cut=layer_cut)

        graph = gs.import_onnx(model)
        n_to_trim = []
        for node in graph.nodes:
            if node.name == layer_cut:
                to_trim = True
                last_node = node
            if to_trim:
                n_to_trim.append(node)
        for node in n_to_trim:
            last_node.outputs = node.outputs
            node.outputs.clear()
        # create a new output for the graph
        # and remove the previous one
        v = gs.Variable("features", dtype=np.float32, shape=out_shape)
        graph.outputs = [v]
        last_node.outputs = graph.outputs
        logger.debug(f"Replacing output of graph with new node {v} of shape {v.shape}")
        graph.cleanup()
        model_proto = gs.export_onnx(graph)
        return model_proto

    def trim_model(self, return_nodes: dict[str, str]):
        # return_nodes should be a  dict['source_layer']='pipeline_name'
        for layer_cut, pipeline_name in return_nodes.items():
            model = self.variants.get_variant_model("original")
            self.variants.safe_onnx_compute(
                f=self._trim_model,
                variant_name=pipeline_name,
                **{
                    "model": model,
                    "layer_cut": layer_cut,
                },
            )

    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:
        device = x.device
        providers = ["CPUExecutionProvider", "CUDAExecutionProvider"]
        if device == torch.device("cpu"):
            device_type = "cpu"
        else:
            device_type = "cuda"
        ort_sessions = {}
        outputs = {}
        batch_size = x.size()[0]
        for variant in self.variants.get_only_modified_variants().keys():
            path = self.variants.get_variant_path(variant)
            logger.debug(f"Performing ONNX inference of {variant} located at {path}")
            ort_sessions[variant] = rt.InferenceSession(path, providers=providers)
            input_of_sess = ort_sessions[variant].get_inputs()[0]
            output_of_sess = ort_sessions[variant].get_outputs()[0]
            input_name = input_of_sess.name
            output_name = output_of_sess.name
            bind = ort_sessions[variant].io_binding()
            match output_of_sess.shape:
                case (_, d):
                    out_shape = (batch_size, d)
                case (_, h, w, d):
                    out_shape = (batch_size, h, w, d)
                case _:
                    assert False

            out_tensor = torch.empty(
                size=torch.Size(out_shape),
                dtype=torch.float32,
                device=device,
            ).contiguous()

            bind.bind_input(
                name=input_name,
                device_type=device_type,
                device_id=0,
                element_type=np.float32,
                shape=tuple(x.shape),
                buffer_ptr=x.data_ptr(),
            )
            bind.bind_output(
                name=output_name,
                device_type=device_type,
                device_id=0,
                element_type=np.float32,
                shape=tuple(out_tensor.shape),
                buffer_ptr=out_tensor.data_ptr(),
            )
            ort_sessions[variant].run_with_iobinding(bind)
            outputs[variant] = out_tensor
        return outputs


@register_model()
def generic_onnx_model(onnx_path: str) -> GenericONNXModel:
    return GenericONNXModel(onnx_path=onnx_path)
